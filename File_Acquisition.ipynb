{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b95d8d1",
   "metadata": {},
   "source": [
    "# Initial Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "427cc289",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bigdata_a3_utils import *\n",
    "import shutil\n",
    "import os\n",
    "from pathlib import Path\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7eee1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = get_cache_directory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd09fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_cache_dir = default_cache_path()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed04b8b9",
   "metadata": {},
   "source": [
    "# Checking availble disk space\n",
    "- Files could also be stored on external drive\n",
    "- Simply change file path in save_path variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04c7723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the disk where your cache directory is located\n",
    "cache_path = cache_dir\n",
    "drive = os.path.splitdrive(cache_path)[0]\n",
    "\n",
    "# Get disk usage statistics\n",
    "total, used, free = shutil.disk_usage(drive)\n",
    "\n",
    "# Convert to GB for easier reading\n",
    "total_gb = total / (1024**3)\n",
    "used_gb = used / (1024**3)\n",
    "free_gb = free / (1024**3)\n",
    "\n",
    "print(f\"Drive {drive}:\")\n",
    "print(f\"Total: {total_gb:.2f} GB\")\n",
    "print(f\"Used: {used_gb:.2f} GB\")\n",
    "print(f\"Free: {free_gb:.2f} GB\")\n",
    "\n",
    "# Check if you have enough space (e.g., 220GB to be safe)\n",
    "required_space = 220  # GB\n",
    "if free_gb > required_space:\n",
    "    print(f\"✅ You have enough space for the 200GB dataset (with some buffer)\")\n",
    "else:\n",
    "    print(f\"❌ Not enough space! You need at least {required_space} GB, but only have {free_gb:.2f} GB free\")\n",
    "    print(f\"Consider setting a custom cache directory on a drive with more space\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac7ab06",
   "metadata": {},
   "source": [
    "# Division of categories into subsets per person:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42ee5c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset list: [['All_Beauty', 'Amazon_Fashion', 'Appliances', 'Arts_Crafts_and_Sewing', 'Automotive', 'Baby_Products', 'Beauty_and_Personal_Care', 'Books', 'CDs_and_Vinyl', 'Cell_Phones_and_Accessories', 'Clothing_Shoes_and_Jewelry'], ['Digital_Music', 'Electronics', 'Gift_Cards', 'Grocery_and_Gourmet_Food', 'Handmade_Products', 'Health_and_Household', 'Health_and_Personal_Care', 'Home_and_Kitchen', 'Industrial_and_Scientific', 'Kindle_Store', 'Magazine_Subscriptions'], ['Movies_and_TV', 'Musical_Instruments', 'Office_Products', 'Patio_Lawn_and_Garden', 'Pet_Supplies', 'Software', 'Sports_and_Outdoors', 'Subscription_Boxes', 'Tools_and_Home_Improvement', 'Toys_and_Games', 'Video_Games', 'Unknown']]\n",
      "Length: 34\n"
     ]
    }
   ],
   "source": [
    "subset_sizes = [11, 11, 12]  # Define the sizes of each subset\n",
    "\n",
    "# Create subsets\n",
    "subsets = []\n",
    "start_index = 0\n",
    "for size in subset_sizes:\n",
    "    subsets.append(VALID_CATEGORIES[start_index:start_index + size])\n",
    "    start_index += size\n",
    "\n",
    "# Unpack the subsets into individual variables\n",
    "kailash_subset, tbd_subset1, tbd_subset2 = subsets\n",
    "\n",
    "print(f\"Subset list: {subsets}\")\n",
    "print(f\"Length: {len(VALID_CATEGORIES)}\")\n",
    "# Output the subsets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818459c5",
   "metadata": {},
   "source": [
    "# Setting up log directory and logging system\n",
    "- Used for proof of file acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7301ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = Path(\"logs\")\n",
    "log_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "logging.basicConfig(\n",
    "    filename=log_dir / 'download_log.log',\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9be841",
   "metadata": {},
   "source": [
    "# Downloading the folders and metadata\n",
    "- Just change the file path in save_path accordingly to where you want it stored\n",
    "- Also change the \"kailash_subset\" to the subset variable assigned from above "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87ce820",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = Path(r\"C:\\BigDataA3\\A3_Dataset\")\n",
    "save_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "for i in range(2):\n",
    "    logging.info(f\"n------Processing {kailash_subset[i]}------\")\n",
    "    print(f\"\\n------Processing {kailash_subset[i]}------\")\n",
    "\n",
    "    download_all_amazon_reviews(base_save_path=save_path, \n",
    "                                categories=[kailash_subset[i]], compress=False)\n",
    "    \n",
    "    review_folder = save_path / f\"raw_review_{kailash_subset[i]}\"\n",
    "    metadata_folder = save_path / f\"raw_metadata_{kailash_subset[i]}\"\n",
    "\n",
    "    if review_folder.exists():\n",
    "        compressed = compress_folder(review_folder, \n",
    "                                     compression_format=\"gz\", \n",
    "                                     level=6)\n",
    "        \n",
    "        logging.info(f\"Compressed reviews to: {compressed}\")\n",
    "        # print(f\"Compressed reviews to: {compressed}\")\n",
    "\n",
    "    if metadata_folder.exists():\n",
    "        compressed = compress_folder(metadata_folder, \n",
    "                                     compression_format=\"gz\", \n",
    "                                     level=6)\n",
    "        \n",
    "        logging.info(f\"Compressed metadata to: {compressed}\")\n",
    "        # print(f\"Compressed metadata to: {compressed}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
