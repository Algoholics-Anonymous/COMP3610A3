{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf1d320e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bigdata_a3_utils import *\n",
    "import shutil\n",
    "import os\n",
    "from pathlib import Path\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad7eee1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Current cache directory: C:\\Users\\kaila\\.cache\\huggingface\\datasets\n",
      "[NOTE] To use a custom cache directory, set HF_DATASETS_CACHE before importing datasets.\n",
      "Example:\n",
      "    import os\n",
      "    os.environ['HF_DATASETS_CACHE'] = 'C:\\\\your\\\\custom\\\\path'\n",
      "    from datasets import load_dataset\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cache_dir = get_cache_directory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebd09fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Your default cache path: \"C:\\Users\\kaila\\.cache\\huggingface\\datasets\"\n"
     ]
    }
   ],
   "source": [
    "default_cache_dir = default_cache_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a04c7723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive C::\n",
      "Total: 937.25 GB\n",
      "Used: 385.16 GB\n",
      "Free: 552.09 GB\n",
      "✅ You have enough space for the 200GB dataset (with some buffer)\n"
     ]
    }
   ],
   "source": [
    "# Get the disk where your cache directory is located\n",
    "cache_path = cache_dir\n",
    "drive = os.path.splitdrive(cache_path)[0]\n",
    "\n",
    "# Get disk usage statistics\n",
    "total, used, free = shutil.disk_usage(drive)\n",
    "\n",
    "# Convert to GB for easier reading\n",
    "total_gb = total / (1024**3)\n",
    "used_gb = used / (1024**3)\n",
    "free_gb = free / (1024**3)\n",
    "\n",
    "print(f\"Drive {drive}:\")\n",
    "print(f\"Total: {total_gb:.2f} GB\")\n",
    "print(f\"Used: {used_gb:.2f} GB\")\n",
    "print(f\"Free: {free_gb:.2f} GB\")\n",
    "\n",
    "# Check if you have enough space (e.g., 220GB to be safe)\n",
    "required_space = 220  # GB\n",
    "if free_gb > required_space:\n",
    "    print(f\"✅ You have enough space for the 200GB dataset (with some buffer)\")\n",
    "else:\n",
    "    print(f\"❌ Not enough space! You need at least {required_space} GB, but only have {free_gb:.2f} GB free\")\n",
    "    print(f\"Consider setting a custom cache directory on a drive with more space\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01cc023f",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_in_dataset = [\n",
    "    \"All_Beauty\", \"Amazon_Fashion\",\n",
    "    \"Appliances\", \"Arts_Crafts_and_Sewing\",\n",
    "    \"Automotive\", \"Baby_Products\",\n",
    "    \"Beauty_and_Personal_Care\", \"Books\",\n",
    "    \"CDs_and_Vinyl\", \"Cell_Phones_and_Accessories\",\n",
    "    \"Clothing_Shoes_and_Jewelry\", \"Digital_Music\",\n",
    "    \"Electronics\", \"Gift_Cards\",\n",
    "    \"Grocery_and_Gourmet_Food\", \"Homemade_Products\",\n",
    "    \"Health_and_Household\", \"Health_and_Personal_Care\",\n",
    "    \"Home_and_Kitchen\", \"Industrial_and_Scientific\",\n",
    "    \"Kindle_Store\", \"Magazine_Subscriptions\",\n",
    "    \"Movies_and_TV\", \"Musical_Instruments\",\n",
    "    \"Office_Products\", \"Patio_Lawn_and_Garden\",\n",
    "    \"Pet_Supplies\", \"Software\",\n",
    "    \"Sports_and_Outdoors\", \"Subsription_Boxes\",\n",
    "    \"Tools_and_Home_Improvement\", \"Toys_and_Games\",\n",
    "    \"Video_Games\", \"Unknown\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf2b107f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n"
     ]
    }
   ],
   "source": [
    "print(len(categories_in_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42ee5c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset list: [['All_Beauty', 'Amazon_Fashion', 'Appliances', 'Arts_Crafts_and_Sewing', 'Automotive', 'Baby_Products'], ['Beauty_and_Personal_Care', 'Books', 'CDs_and_Vinyl', 'Cell_Phones_and_Accessories', 'Clothing_Shoes_and_Jewelry', 'Digital_Music'], ['Electronics', 'Gift_Cards', 'Grocery_and_Gourmet_Food', 'Homemade_Products', 'Health_and_Household', 'Health_and_Personal_Care'], ['Home_and_Kitchen', 'Industrial_and_Scientific', 'Kindle_Store', 'Magazine_Subscriptions', 'Movies_and_TV', 'Musical_Instruments'], ['Office_Products', 'Patio_Lawn_and_Garden', 'Pet_Supplies', 'Software', 'Sports_and_Outdoors', 'Subsription_Boxes'], ['Tools_and_Home_Improvement', 'Toys_and_Games', 'Video_Games', 'Unknown']]\n",
      "Subset 1: 6\n",
      "Subset 2: 6\n",
      "Subset 3: 6\n",
      "Subset 4: 6\n",
      "Subset 5: 6\n",
      "Subset 6: 4\n"
     ]
    }
   ],
   "source": [
    "subset_sizes = [6, 6, 6, 6, 6, 4]  # Define the sizes of each subset\n",
    "\n",
    "# Create subsets\n",
    "subsets = []\n",
    "start_index = 0\n",
    "for size in subset_sizes:\n",
    "    subsets.append(categories_in_dataset[start_index:start_index + size])\n",
    "    start_index += size\n",
    "\n",
    "# Unpack the subsets into individual variables\n",
    "subset1, subset2, subset3, subset4, subset5, subset6 = subsets\n",
    "\n",
    "print(f\"Subset list: {subsets}\")\n",
    "# Output the subsets\n",
    "print(\"Subset 1:\", len(subset1))\n",
    "print(\"Subset 2:\", len(subset2))\n",
    "print(\"Subset 3:\", len(subset3))\n",
    "print(\"Subset 4:\", len(subset4))\n",
    "print(\"Subset 5:\", len(subset5))\n",
    "print(\"Subset 6:\", len(subset6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7301ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = Path(\"logs\")\n",
    "log_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "logging.basicConfig(\n",
    "    filename=log_dir / 'download_log.log',\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87ce820",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = Path(r\"C:\\Users\\kaila\\A3_dataset\")\n",
    "\n",
    "for i in range(6):\n",
    "    logging.info(f\"n------Processing {subset1[i]}------\")\n",
    "    print(f\"\\n------Processing {subset1[i]}------\")\n",
    "\n",
    "    download_all_amazon_reviews(base_save_path=save_path, \n",
    "                                categories=[subset1[i]], compress=False)\n",
    "    \n",
    "    review_folder = save_path / f\"raw_review_{subset1[i]}\"\n",
    "    metadata_folder = save_path / f\"raw_metadata_{subset1[i]}\"\n",
    "\n",
    "    if review_folder.exists():\n",
    "        compressed = compress_folder(review_folder, \n",
    "                                     compression_format=\"gz\", \n",
    "                                     level=6)\n",
    "        \n",
    "        logging.info(f\"Compressed reviews to: {compressed}\")\n",
    "        print(f\"Compressed reviews to: {compressed}\")\n",
    "\n",
    "    if metadata_folder.exists():\n",
    "        compressed = compress_folder(metadata_folder, \n",
    "                                     compression_format=\"gz\", \n",
    "                                     level=6)\n",
    "        \n",
    "        logging.info(f\"Compressed metadata to: {compressed}\")\n",
    "        print(f\"Compressed metadata to: {compressed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60c50a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_path = Path(r\"C:\\Users\\kaila\\A3_dataset\")\n",
    "\n",
    "# for category in categories_in_dataset:\n",
    "#     logging.info(f\"n------Processing {category}------\")\n",
    "#     print(f\"\\n------Processing {category}------\")\n",
    "\n",
    "#     download_all_amazon_reviews(base_save_path=save_path, \n",
    "#                                 categories=[category], compress=False)\n",
    "    \n",
    "#     review_folder = save_path / f\"raw_review_{category}\"\n",
    "#     metadata_folder = save_path / f\"raw_metadata_{category}\"\n",
    "\n",
    "#     if review_folder.exists():\n",
    "#         compressed = compress_folder(review_folder, \n",
    "#                                      compression_format=\"gz\", \n",
    "#                                      level=6)\n",
    "        \n",
    "#         logging.info(f\"Compressed reviews to: {compressed}\")\n",
    "#         print(f\"Compressed reviews to: {compressed}\")\n",
    "\n",
    "#     if metadata_folder.exists():\n",
    "#         compressed = compress_folder(metadata_folder, \n",
    "#                                      compression_format=\"gz\", \n",
    "#                                      level=6)\n",
    "        \n",
    "#         logging.info(f\"Compressed metadata to: {compressed}\")\n",
    "#         print(f\"Compressed metadata to: {compressed}\")\n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
