{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b95d8d1",
   "metadata": {},
   "source": [
    "# Initial Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427cc289",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bigdata_a3_utils import *\n",
    "import shutil\n",
    "import os\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import tarfile\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7eee1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = get_cache_directory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd09fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_cache_dir = default_cache_path()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed04b8b9",
   "metadata": {},
   "source": [
    "# Checking availble disk space\n",
    "- Files could also be stored on external drive\n",
    "- Simply change file path in save_path variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04c7723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the disk where your cache directory is located\n",
    "cache_path = cache_dir\n",
    "drive = os.path.splitdrive(cache_path)[0]\n",
    "\n",
    "# Get disk usage statistics\n",
    "total, used, free = shutil.disk_usage(drive)\n",
    "\n",
    "# Convert to GB for easier reading\n",
    "total_gb = total / (1024**3)\n",
    "used_gb = used / (1024**3)\n",
    "free_gb = free / (1024**3)\n",
    "\n",
    "print(f\"Drive {drive}:\")\n",
    "print(f\"Total: {total_gb:.2f} GB\")\n",
    "print(f\"Used: {used_gb:.2f} GB\")\n",
    "print(f\"Free: {free_gb:.2f} GB\")\n",
    "\n",
    "# Check if you have enough space (e.g., 220GB to be safe)\n",
    "required_space = 220  # GB\n",
    "if free_gb > required_space:\n",
    "    print(f\"✅ You have enough space for the 200GB dataset (with some buffer)\")\n",
    "else:\n",
    "    print(f\"❌ Not enough space! You need at least {required_space} GB, but only have {free_gb:.2f} GB free\")\n",
    "    print(f\"Consider setting a custom cache directory on a drive with more space\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac7ab06",
   "metadata": {},
   "source": [
    "# Division of categories into subsets per person:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ee5c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_sizes = [11, 11, 12]  # Define the sizes of each subset\n",
    "\n",
    "# Create subsets\n",
    "subsets = []\n",
    "start_index = 0\n",
    "for size in subset_sizes:\n",
    "    subsets.append(VALID_CATEGORIES[start_index:start_index + size])\n",
    "    start_index += size\n",
    "\n",
    "# Unpack the subsets into individual variables\n",
    "kailash_subset, tbd_subset1, tbd_subset2 = subsets\n",
    "\n",
    "print(f\"Subset list: {subsets}\")\n",
    "print(f\"Length: {len(VALID_CATEGORIES)}\")\n",
    "# Output the subsets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818459c5",
   "metadata": {},
   "source": [
    "# Setting up log directory and logging system\n",
    "- Used for proof of file acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7301ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = Path(\"logs\")\n",
    "log_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "logging.basicConfig(\n",
    "    filename=log_dir / 'download_log.log',\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c322bf70",
   "metadata": {},
   "source": [
    "# Error handling:\n",
    "- In the event of an issue where the folder was not dowloaded properly, as reflected by the output in this notebook, use the cell below while specifying the category that failed to download in the category variable. \n",
    "- In the event of a issue with preprocessing any particular category (OS error, Permissions error, Temp file being deleted before full extraction) it is likely the initial download was corrupted by some means. As such simply redownload via the cell below, again specifying the failed category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190d3cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_compressed_file(file_path):\n",
    "    \"\"\"\n",
    "    Verifies if a compressed file (tar.gz, zip, etc.) is valid and not corrupted.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the compressed file\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if the file is valid, False otherwise\n",
    "    \"\"\"\n",
    "    file_path = Path(file_path)\n",
    "    \n",
    "    if not file_path.exists():\n",
    "        print(f\"Error: File does not exist: {file_path}\")\n",
    "        return False\n",
    "        \n",
    "    try:\n",
    "        if str(file_path).endswith('.tar.gz') or str(file_path).endswith('.tgz'):\n",
    "            # Test if tar.gz file is valid\n",
    "            with tarfile.open(file_path, 'r:gz') as tar:\n",
    "                # Just check the integrity by attempting to list contents\n",
    "                tar.getnames()\n",
    "                print(f\"✓ Verified {file_path} is a valid tar.gz file\")\n",
    "                return True\n",
    "                \n",
    "        elif str(file_path).endswith('.zip'):\n",
    "            # Test if zip file is valid\n",
    "            with zipfile.ZipFile(file_path) as zip_file:\n",
    "                # Check the integrity of the zip file\n",
    "                result = zip_file.testzip()\n",
    "                if result is None:\n",
    "                    print(f\"✓ Verified {file_path} is a valid zip file\")\n",
    "                    return True\n",
    "                else:\n",
    "                    print(f\"Error: {file_path} is corrupted. First bad file: {result}\")\n",
    "                    return False\n",
    "        else:\n",
    "            print(f\"Error: Unsupported file format for {file_path}\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error verifying {file_path}: {str(e)}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9be841",
   "metadata": {},
   "source": [
    "# Downloading the folders and metadata\n",
    "- Just change the file path in save_path accordingly to where you want it stored\n",
    "- Also change the \"kailash_subset\" to the subset variable assigned from above "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87ce820",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = Path(r\"C:\\BigDataA3\\A3_Dataset\")\n",
    "save_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for category in kailash_subset:\n",
    "    logging.info(f\"n------Processing {category}------\")\n",
    "    print(f\"\\n------Processing {category}------\")\n",
    "\n",
    "    download_all_amazon_reviews(base_save_path=save_path, \n",
    "                                categories=[category], compress=False)\n",
    "\n",
    "    review_folder = save_path / f\"raw_review_{category}\"\n",
    "    metadata_folder = save_path / f\"raw_meta_{category}\"\n",
    "\n",
    "    if review_folder.exists():\n",
    "        compressed = compress_folder(review_folder, \n",
    "                                        compression_format=\"gz\", \n",
    "                                        level=6)\n",
    "        \n",
    "        logging.info(f\"Compressed reviews to: {compressed}\")\n",
    "        print(f\"Compressed reviews to: {compressed}\")\n",
    "\n",
    "    if metadata_folder.exists():\n",
    "        compressed = compress_folder(metadata_folder, \n",
    "                                        compression_format=\"gz\", \n",
    "                                        level=6)\n",
    "        \n",
    "        logging.info(f\"Compressed metadata to: {compressed}\")\n",
    "        print(f\"Compressed metadata to: {compressed}\")\n",
    "\n",
    "    \n",
    "    verify_compressed_file(review_folder)\n",
    "    verify_compressed_file(metadata_folder) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9259a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "category = \"\"\n",
    "\n",
    "print(f\"\\n------Processing {category}------\")\n",
    "\n",
    "download_all_amazon_reviews(base_save_path=save_path, \n",
    "                            categories=[category], compress=False)\n",
    "\n",
    "review_folder = save_path / f\"raw_review_{category}\"\n",
    "metadata_folder = save_path / f\"raw_meta_{category}\"\n",
    "\n",
    "if review_folder.exists():\n",
    "    compressed = compress_folder(review_folder, \n",
    "                                    compression_format=\"gz\", \n",
    "                                    level=6)\n",
    "    \n",
    "\n",
    "    print(f\"Compressed reviews to: {compressed}\")\n",
    "\n",
    "if metadata_folder.exists():\n",
    "    compressed = compress_folder(metadata_folder, \n",
    "                                    compression_format=\"gz\", \n",
    "                                    level=6)\n",
    "    \n",
    "    \n",
    "    print(f\"Compressed metadata to: {compressed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541cfa04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
